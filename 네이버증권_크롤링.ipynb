{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####################################################################################################\n",
      "크롤링 뉴스 종목 리스트 :  ['씨에스윈드']\n",
      "####################################################################################################\n"
     ]
    },
    {
     "ename": "UnicodeEncodeError",
     "evalue": "'ascii' codec can't encode characters in position 29-33: ordinal not in range(128)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeEncodeError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 146\u001b[0m\n\u001b[0;32m    143\u001b[0m         tmp \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m    145\u001b[0m \u001b[39m# page 401이상 있어도 기사목록이 안바뀌므로 max 400으로 설정\u001b[39;00m\n\u001b[1;32m--> 146\u001b[0m last_page \u001b[39m=\u001b[39m \u001b[39mmin\u001b[39m(\u001b[39m400\u001b[39m, \u001b[39mint\u001b[39m(get_last_page()))\n\u001b[0;32m    147\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mLast page : \u001b[39m\u001b[39m{\u001b[39;00mlast_page\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m    148\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, last_page \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m):\n",
      "Cell \u001b[1;32mIn[5], line 19\u001b[0m, in \u001b[0;36mget_last_page\u001b[1;34m(p)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     16\u001b[0m     \u001b[39m# 내용 기준 naver_crawler\u001b[39;00m\n\u001b[0;32m     17\u001b[0m     main_url \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mhttps://finance.naver.com/item/news_news.nhn?code=\u001b[39m\u001b[39m{\u001b[39;00mcode\u001b[39m}\u001b[39;00m\u001b[39m&page=\u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(\n\u001b[0;32m     18\u001b[0m         p) \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m&sm=entity_id.basic&clusterId=\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m---> 19\u001b[0m soup \u001b[39m=\u001b[39m get_html(main_url)\n\u001b[0;32m     20\u001b[0m last_btn \u001b[39m=\u001b[39m soup\u001b[39m.\u001b[39mfind(\u001b[39m'\u001b[39m\u001b[39mtd\u001b[39m\u001b[39m'\u001b[39m, class_\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mpgRR\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     21\u001b[0m \u001b[39mif\u001b[39;00m last_btn \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "Cell \u001b[1;32mIn[5], line 33\u001b[0m, in \u001b[0;36mget_html\u001b[1;34m(u)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_html\u001b[39m(u):\n\u001b[0;32m     31\u001b[0m     \u001b[39m# print('get_html시작')\u001b[39;00m\n\u001b[0;32m     32\u001b[0m     url \u001b[39m=\u001b[39m u\n\u001b[1;32m---> 33\u001b[0m     html \u001b[39m=\u001b[39m urlopen(url)\n\u001b[0;32m     34\u001b[0m     soup \u001b[39m=\u001b[39m BeautifulSoup(html, \u001b[39m'\u001b[39m\u001b[39mhtml.parser\u001b[39m\u001b[39m'\u001b[39m, from_encoding\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mcp949\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     35\u001b[0m     \u001b[39mreturn\u001b[39;00m soup\n",
      "File \u001b[1;32mc:\\Users\\Hyomin\\anaconda3\\envs\\data32\\lib\\urllib\\request.py:216\u001b[0m, in \u001b[0;36murlopen\u001b[1;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    215\u001b[0m     opener \u001b[39m=\u001b[39m _opener\n\u001b[1;32m--> 216\u001b[0m \u001b[39mreturn\u001b[39;00m opener\u001b[39m.\u001b[39;49mopen(url, data, timeout)\n",
      "File \u001b[1;32mc:\\Users\\Hyomin\\anaconda3\\envs\\data32\\lib\\urllib\\request.py:519\u001b[0m, in \u001b[0;36mOpenerDirector.open\u001b[1;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[0;32m    516\u001b[0m     req \u001b[39m=\u001b[39m meth(req)\n\u001b[0;32m    518\u001b[0m sys\u001b[39m.\u001b[39maudit(\u001b[39m'\u001b[39m\u001b[39murllib.Request\u001b[39m\u001b[39m'\u001b[39m, req\u001b[39m.\u001b[39mfull_url, req\u001b[39m.\u001b[39mdata, req\u001b[39m.\u001b[39mheaders, req\u001b[39m.\u001b[39mget_method())\n\u001b[1;32m--> 519\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_open(req, data)\n\u001b[0;32m    521\u001b[0m \u001b[39m# post-process response\u001b[39;00m\n\u001b[0;32m    522\u001b[0m meth_name \u001b[39m=\u001b[39m protocol\u001b[39m+\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m_response\u001b[39m\u001b[39m\"\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Hyomin\\anaconda3\\envs\\data32\\lib\\urllib\\request.py:536\u001b[0m, in \u001b[0;36mOpenerDirector._open\u001b[1;34m(self, req, data)\u001b[0m\n\u001b[0;32m    533\u001b[0m     \u001b[39mreturn\u001b[39;00m result\n\u001b[0;32m    535\u001b[0m protocol \u001b[39m=\u001b[39m req\u001b[39m.\u001b[39mtype\n\u001b[1;32m--> 536\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_chain(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhandle_open, protocol, protocol \u001b[39m+\u001b[39;49m\n\u001b[0;32m    537\u001b[0m                           \u001b[39m'\u001b[39;49m\u001b[39m_open\u001b[39;49m\u001b[39m'\u001b[39;49m, req)\n\u001b[0;32m    538\u001b[0m \u001b[39mif\u001b[39;00m result:\n\u001b[0;32m    539\u001b[0m     \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\Users\\Hyomin\\anaconda3\\envs\\data32\\lib\\urllib\\request.py:496\u001b[0m, in \u001b[0;36mOpenerDirector._call_chain\u001b[1;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[0;32m    494\u001b[0m \u001b[39mfor\u001b[39;00m handler \u001b[39min\u001b[39;00m handlers:\n\u001b[0;32m    495\u001b[0m     func \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(handler, meth_name)\n\u001b[1;32m--> 496\u001b[0m     result \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs)\n\u001b[0;32m    497\u001b[0m     \u001b[39mif\u001b[39;00m result \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    498\u001b[0m         \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\Users\\Hyomin\\anaconda3\\envs\\data32\\lib\\urllib\\request.py:1391\u001b[0m, in \u001b[0;36mHTTPSHandler.https_open\u001b[1;34m(self, req)\u001b[0m\n\u001b[0;32m   1390\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mhttps_open\u001b[39m(\u001b[39mself\u001b[39m, req):\n\u001b[1;32m-> 1391\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdo_open(http\u001b[39m.\u001b[39;49mclient\u001b[39m.\u001b[39;49mHTTPSConnection, req,\n\u001b[0;32m   1392\u001b[0m         context\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_context, check_hostname\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_check_hostname)\n",
      "File \u001b[1;32mc:\\Users\\Hyomin\\anaconda3\\envs\\data32\\lib\\urllib\\request.py:1348\u001b[0m, in \u001b[0;36mAbstractHTTPHandler.do_open\u001b[1;34m(self, http_class, req, **http_conn_args)\u001b[0m\n\u001b[0;32m   1346\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1347\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1348\u001b[0m         h\u001b[39m.\u001b[39;49mrequest(req\u001b[39m.\u001b[39;49mget_method(), req\u001b[39m.\u001b[39;49mselector, req\u001b[39m.\u001b[39;49mdata, headers,\n\u001b[0;32m   1349\u001b[0m                   encode_chunked\u001b[39m=\u001b[39;49mreq\u001b[39m.\u001b[39;49mhas_header(\u001b[39m'\u001b[39;49m\u001b[39mTransfer-encoding\u001b[39;49m\u001b[39m'\u001b[39;49m))\n\u001b[0;32m   1350\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mOSError\u001b[39;00m \u001b[39mas\u001b[39;00m err: \u001b[39m# timeout error\u001b[39;00m\n\u001b[0;32m   1351\u001b[0m         \u001b[39mraise\u001b[39;00m URLError(err)\n",
      "File \u001b[1;32mc:\\Users\\Hyomin\\anaconda3\\envs\\data32\\lib\\http\\client.py:1282\u001b[0m, in \u001b[0;36mHTTPConnection.request\u001b[1;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[0;32m   1279\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrequest\u001b[39m(\u001b[39mself\u001b[39m, method, url, body\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, headers\u001b[39m=\u001b[39m{}, \u001b[39m*\u001b[39m,\n\u001b[0;32m   1280\u001b[0m             encode_chunked\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m   1281\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Send a complete request to the server.\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1282\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_send_request(method, url, body, headers, encode_chunked)\n",
      "File \u001b[1;32mc:\\Users\\Hyomin\\anaconda3\\envs\\data32\\lib\\http\\client.py:1293\u001b[0m, in \u001b[0;36mHTTPConnection._send_request\u001b[1;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[0;32m   1290\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39maccept-encoding\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m header_names:\n\u001b[0;32m   1291\u001b[0m     skips[\u001b[39m'\u001b[39m\u001b[39mskip_accept_encoding\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m-> 1293\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mputrequest(method, url, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mskips)\n\u001b[0;32m   1295\u001b[0m \u001b[39m# chunked encoding will happen if HTTP/1.1 is used and either\u001b[39;00m\n\u001b[0;32m   1296\u001b[0m \u001b[39m# the caller passes encode_chunked=True or the following\u001b[39;00m\n\u001b[0;32m   1297\u001b[0m \u001b[39m# conditions hold:\u001b[39;00m\n\u001b[0;32m   1298\u001b[0m \u001b[39m# 1. content-length has not been explicitly set\u001b[39;00m\n\u001b[0;32m   1299\u001b[0m \u001b[39m# 2. the body is a file or iterable, but not a str or bytes-like\u001b[39;00m\n\u001b[0;32m   1300\u001b[0m \u001b[39m# 3. Transfer-Encoding has NOT been explicitly set by the caller\u001b[39;00m\n\u001b[0;32m   1302\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mcontent-length\u001b[39m\u001b[39m'\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m header_names:\n\u001b[0;32m   1303\u001b[0m     \u001b[39m# only chunk body if not explicitly set for backwards\u001b[39;00m\n\u001b[0;32m   1304\u001b[0m     \u001b[39m# compatibility, assuming the client code is already handling the\u001b[39;00m\n\u001b[0;32m   1305\u001b[0m     \u001b[39m# chunking\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Hyomin\\anaconda3\\envs\\data32\\lib\\http\\client.py:1131\u001b[0m, in \u001b[0;36mHTTPConnection.putrequest\u001b[1;34m(self, method, url, skip_host, skip_accept_encoding)\u001b[0m\n\u001b[0;32m   1127\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_path(url)\n\u001b[0;32m   1129\u001b[0m request \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m \u001b[39m%\u001b[39m (method, url, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_http_vsn_str)\n\u001b[1;32m-> 1131\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_encode_request(request))\n\u001b[0;32m   1133\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_http_vsn \u001b[39m==\u001b[39m \u001b[39m11\u001b[39m:\n\u001b[0;32m   1134\u001b[0m     \u001b[39m# Issue some standard headers for better HTTP/1.1 compliance\u001b[39;00m\n\u001b[0;32m   1136\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m skip_host:\n\u001b[0;32m   1137\u001b[0m         \u001b[39m# this header is issued *only* for HTTP/1.1\u001b[39;00m\n\u001b[0;32m   1138\u001b[0m         \u001b[39m# connections. more specifically, this means it is\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1148\u001b[0m         \u001b[39m# but the host of the actual URL, not the host of the\u001b[39;00m\n\u001b[0;32m   1149\u001b[0m         \u001b[39m# proxy.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Hyomin\\anaconda3\\envs\\data32\\lib\\http\\client.py:1211\u001b[0m, in \u001b[0;36mHTTPConnection._encode_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m   1209\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_encode_request\u001b[39m(\u001b[39mself\u001b[39m, request):\n\u001b[0;32m   1210\u001b[0m     \u001b[39m# ASCII also helps prevent CVE-2019-9740.\u001b[39;00m\n\u001b[1;32m-> 1211\u001b[0m     \u001b[39mreturn\u001b[39;00m request\u001b[39m.\u001b[39;49mencode(\u001b[39m'\u001b[39;49m\u001b[39mascii\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "\u001b[1;31mUnicodeEncodeError\u001b[0m: 'ascii' codec can't encode characters in position 29-33: ordinal not in range(128)"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import re\n",
    "from multiprocessing import Process\n",
    "from urllib.request import urlopen\n",
    "from datetime import datetime\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def get_last_page(p=1):\n",
    "    # print('get_last_page 시작')\n",
    "    global code\n",
    "    if title_entity == True:\n",
    "        # 제목 기준 naver_crawler\n",
    "        main_url = f'https://finance.naver.com/item/news_news.nhn?code={code}&page=' + str(\n",
    "            p) + '&sm=title_entity_id.basic&clusterId='\n",
    "    else:\n",
    "        # 내용 기준 naver_crawler\n",
    "        main_url = f'https://finance.naver.com/item/news_news.nhn?code={code}&page=' + str(\n",
    "            p) + '&sm=entity_id.basic&clusterId='\n",
    "    soup = get_html(main_url)\n",
    "    last_btn = soup.find('td', class_='pgRR')\n",
    "    if last_btn is None:\n",
    "        return p\n",
    "    else:\n",
    "        last_link = last_btn.a['href']\n",
    "        split1 = last_link.split('=')\n",
    "        split2 = split1[2].split('&')\n",
    "        last_page = split2[0]\n",
    "        return get_last_page(last_page)\n",
    "\n",
    "def get_html(u):\n",
    "    # print('get_html시작')\n",
    "    url = u\n",
    "    html = urlopen(url)\n",
    "    soup = BeautifulSoup(html, 'html.parser', from_encoding='cp949')\n",
    "    return soup\n",
    "\n",
    "def get_url(page):\n",
    "    # print('get_url시작')\n",
    "    global code\n",
    "    global title_entity\n",
    "    if title_entity == True:\n",
    "        # 제목 기준 naver_crawler\n",
    "        main_url = f'https://finance.naver.com/item/news_news.nhn?code={code}&page=' + str(\n",
    "            page) + '&sm=title_entity_id.basic&clusterId='\n",
    "    else:\n",
    "        # 내용 기준 naver_crawler\n",
    "        main_url = f'https://finance.naver.com/item/news_news.nhn?code={code}&page=' + str(\n",
    "            page) + '&sm=entity_id.basic&clusterId='\n",
    "    print(main_url)\n",
    "    soup = get_html(main_url)\n",
    "    table = soup.find('table', attrs={'class': 'type5'})\n",
    "    test_list = []\n",
    "    # class=relation_lst로 시작하는 행을 삭제\n",
    "    for tag in table.select('tr[class^=\"relation_lst\"]'):\n",
    "        tag.decompose()\n",
    "    # for문을 돌면서 테이블에서 값을 가져옴\n",
    "    for row in table.find_all('tr'):\n",
    "        cells = row.find_all('td')\n",
    "        if len(cells) == 3:\n",
    "            # href = 뉴스 기사 주소\n",
    "            # cells[1] = 신문사 이름\n",
    "            # cells[2] = 뉴스 입력 시간\n",
    "            # href_list.append(cells[0].a['href'])\n",
    "            # date_list.append(cells[2].text)\n",
    "            test_list.append([cells[2].text, cells[0].a['href']])\n",
    "    return test_list\n",
    "\n",
    "\n",
    "def read_article_cnn(l, code, title_entity):\n",
    "    # print('read_arti_cnn 시작')\n",
    "    # print(l)\n",
    "    news_url = 'https://finance.naver.com'\n",
    "    article_text = []\n",
    "    if l != None:\n",
    "        article_url = news_url + l[1]\n",
    "        html = get_html(article_url)\n",
    "        news_read = html.find('div', class_='scr01')\n",
    "        for tag in news_read.select('div[class=link_news]'):\n",
    "            tag.decompose()\n",
    "        for tag in news_read.select('ul'):\n",
    "            tag.decompose()\n",
    "        for tag in news_read.select('strong'):\n",
    "            tag.decompose()\n",
    "        for tag in news_read.select('table'):\n",
    "            tag.decompose()\n",
    "        for tag in news_read.select('a'):\n",
    "            tag.decompose()\n",
    "        text = news_read.text\n",
    "        text2 = text.split('.')\n",
    "        for s in range(len(text2)-1, 0, -1):\n",
    "            if '@' in text2[s]:\n",
    "                for j in range(len(text2) - 1, s - 1, -1):\n",
    "                    text2.pop(j)\n",
    "        for s in range(len(text2) - 1, 0, -1):\n",
    "            if '한경로보뉴스' in text2[s]:\n",
    "                for j in range(len(text2) - 1, s - 1, -1):\n",
    "                    text2.pop(j)\n",
    "        text3 = ('.').join(text2)\n",
    "        text4 = re.sub('\\[.+?\\]', '', text3, 0).strip()\n",
    "        article_text.append([l[0], text4 + '.'])\n",
    "        print(article_text)\n",
    "        if title_entity:\n",
    "            f = open(f'title_{code}.csv', 'a', -1, encoding='utf-8', newline='')\n",
    "            wr = csv.writer(f)\n",
    "\n",
    "            wr.writerow(article_text[0])\n",
    "            f.close()\n",
    "        else:\n",
    "            f = open(f'contents_{code}.csv', 'a', encoding='utf-8', newline='')\n",
    "            wr = csv.writer(f)\n",
    "            if type(article_text[0][0]) == datetime:\n",
    "                wr.writerow([article_text[0][0], article_text[0][1]])\n",
    "                f.close()\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    stockcode_lst = []\n",
    "    while True:\n",
    "\n",
    "        itemname = input('크롤링 희망하는 종목 코드 :')\n",
    "        stockcode_lst.append(itemname)\n",
    "        tmp = input('종목코드 더 추가하기(Y) | 그만하고 크롤링 시작(N)')\n",
    "        if tmp.lower()=='n':\n",
    "            break\n",
    "\n",
    "    print('#'*100)\n",
    "    print('크롤링 뉴스 종목 리스트 : ',stockcode_lst)\n",
    "    print('#' * 100)\n",
    "    for code in stockcode_lst:\n",
    "        tmp = False\n",
    "        # True : 제목 기준 클러스터링 False : 내용 기준 검색\n",
    "        while not tmp:\n",
    "            ans = 'y'#str(input('뉴스 공시 정렬 기준 (제목(Y), 내용(N)) : ')).lower()\n",
    "            if ans =='y':\n",
    "                title_entity=True\n",
    "                tmp = True\n",
    "            elif ans =='n':\n",
    "                title_entity = False\n",
    "                tmp = True\n",
    "            else:\n",
    "                print('정확히 입력해주세요.')\n",
    "                tmp = False\n",
    "\n",
    "        # page 401이상 있어도 기사목록이 안바뀌므로 max 400으로 설정\n",
    "        last_page = min(400, int(get_last_page()))\n",
    "        print(f'Last page : {last_page}')\n",
    "        for i in range(1, last_page + 1):\n",
    "            test = get_url(i)\n",
    "            procs = []\n",
    "\n",
    "            for index, number in enumerate(test):\n",
    "                # print(number)\n",
    "                proc = Process(target=read_article_cnn, args=(number, code, title_entity))\n",
    "                procs.append(proc)\n",
    "                proc.start()\n",
    "                # print(number[0])\n",
    "\n",
    "            for proc in procs:\n",
    "                proc.join()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data32",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
